{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**CS231A Final Project**"
      ],
      "metadata": {
        "id": "DaM8iR9GNBat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "QLPnvB-BNtQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "connect to drive"
      ],
      "metadata": {
        "id": "pPHMA4pOSMwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "path = \"/content\" # /content is pretty much the root. you can choose other path in your colab workspace\n",
        "os.chdir(path)"
      ],
      "metadata": {
        "id": "YPfUDsZSZg5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# enter the foldername in your Drive where you have saved the .py files in 'p3/code' needed for this problem\n",
        "# e.g. 'cs231a/monocular_depth_estimation':\n",
        "FOLDERNAME = 'cs231a_final_pj'\n",
        "\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "%ls .\n",
        "%cd drive/MyDrive\n",
        "%cd $FOLDERNAME"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf8SaikjSRrI",
        "outputId": "620583f8-bcf8-43ae-f657-80bed7ca3a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/drive/MyDrive\n",
            "/content/drive/.shortcut-targets-by-id/1T0U6l3HfS0JC4GM27mZEFfQKRMYMPHTH/cs231a_final_pj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToLmTjUCaURW",
        "outputId": "5cfddba7-4d2e-49cd-af22-7694ae985359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1T0U6l3HfS0JC4GM27mZEFfQKRMYMPHTH/cs231a_final_pj\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZ0rJBBGJy1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d8dac01-c3ad-493f-dbea-06b9a723d34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version:  1.10.0+cu111\n",
            "Torchvision Version:  0.11.1+cu111\n",
            "Using the GPU!\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import time\n",
        "import itertools\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import matplotlib.image\n",
        "from PIL import Image\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n",
        "# Detect if we have a GPU available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using the GPU!\")\n",
        "else:\n",
        "    print(\"WARNING: Could not find GPU! Using CPU only.\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((512, 1024)),\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the label training images\n",
        "def load_data_labelID(path_label, subfolder, transform, batch_size, shuffle=False):\n",
        "    # create the label dataset\n",
        "    dataset = datasets.ImageFolder(path_label, transform)\n",
        "    index = dataset.class_to_idx[subfolder]\n",
        "    n = 0\n",
        "    m = 0\n",
        "    \n",
        "    for i in range(dataset.__len__()):\n",
        "        if index != dataset.imgs[n][1]:\n",
        "            del dataset.imgs[n]\n",
        "            n = n - 1\n",
        "        else:\n",
        "            if m % 3 != 2:\n",
        "                \n",
        "                del dataset.imgs[n]\n",
        "                n = n - 1\n",
        "            m = m + 1\n",
        "        n = n + 1\n",
        "    len_dataset = dataset.__len__()\n",
        "    Dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
        "    return (len_dataset, Dataloader)\n",
        "\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "\n",
        "    def __call__(self, pic):\n",
        "        return torch.tensor(np.array(pic))\n",
        "\n",
        "\n",
        "def load_data_labelID_test(path_label, subfolder, transform, batch_size, shuffle=False):\n",
        "    # create the label dataset\n",
        "\n",
        "    def pil_loader(path):\n",
        "      # with open(path, 'rb') as f:\n",
        "      img = Image.open(path)\n",
        "      # print(np.array(img))\n",
        "      return img\n",
        "\n",
        "    my_transform = transforms.Compose([ToTensor()])\n",
        "  \n",
        "    dataset = datasets.ImageFolder(path_label, transform=my_transform, loader=pil_loader)\n",
        "    index = dataset.class_to_idx[subfolder]\n",
        "    n = 0\n",
        "    m = 0\n",
        "    \n",
        "    for i in range(dataset.__len__()):\n",
        "        if index != dataset.imgs[n][1]:\n",
        "            del dataset.imgs[n]\n",
        "            n = n - 1\n",
        "        else:\n",
        "            if m % 3 != 2:\n",
        "                \n",
        "                del dataset.imgs[n]\n",
        "                n = n - 1\n",
        "            m = m + 1\n",
        "        n = n + 1\n",
        "    len_dataset = dataset.__len__()\n",
        "    Dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
        "    return (len_dataset, Dataloader)\n",
        "\n",
        "\n",
        "# extract the label training images\n",
        "def load_data_label(path_label, subfolder, transform, batch_size, shuffle=False):\n",
        "    # create the label dataset\n",
        "    dataset = datasets.ImageFolder(path_label, transform)\n",
        "    index = dataset.class_to_idx[subfolder]\n",
        "    n = 0\n",
        "    m = 0\n",
        "    for i in range(dataset.__len__()):\n",
        "        if index != dataset.imgs[n][1]:\n",
        "            del dataset.imgs[n]\n",
        "            n = n - 1\n",
        "        else:\n",
        "            if m % 3 != 0:\n",
        "                del dataset.imgs[n]\n",
        "                n = n - 1\n",
        "            m = m + 1\n",
        "        n = n + 1\n",
        "    len_dataset = dataset.__len__()\n",
        "    Dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
        "    return (len_dataset, Dataloader)\n",
        "\n",
        "\n",
        "# extract raw training images\n",
        "def load_data_raw(path, subfolder, transform, batch_size, shuffle=False):\n",
        "    dataset = datasets.ImageFolder(path, transform)\n",
        "    index = dataset.class_to_idx[subfolder]\n",
        "    n = 0\n",
        "    for i in range(dataset.__len__()):\n",
        "        if index != dataset.imgs[n][1]:\n",
        "            del dataset.imgs[n]\n",
        "            n = n - 1\n",
        "        n = n + 1\n",
        "    len_dataset = dataset.__len__()\n",
        "    Dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
        "    return (len_dataset, Dataloader)"
      ],
      "metadata": {
        "id": "1YCrYj3cR7d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_train = 1\n",
        "\n",
        "(len_train_label, train_label_loader) = load_data_labelID(\n",
        "    'data/gtFine', \n",
        "    'train', transform, batch_size_train, shuffle=False\n",
        ")\n",
        "(len_train_raw, train_raw_loader) = load_data_raw(\n",
        "    'data/leftImg8bit',\n",
        "    'train', transform, batch_size_train, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "1XdQWcv1RnMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "PrdZuRvNN04d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.conv(input)\n",
        "\n",
        "# define U-net\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        self.conv1 = DoubleConv(in_ch, 64)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = DoubleConv(64, 128)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.conv3 = DoubleConv(128, 256)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "        self.conv4 = DoubleConv(256, 512)\n",
        "        self.pool4 = nn.MaxPool2d(2)\n",
        "        self.conv5 = DoubleConv(512, 1024)\n",
        "        self.up6 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
        "        self.conv6 = DoubleConv(1024, 512)\n",
        "        self.up7 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
        "        self.conv7 = DoubleConv(512, 256)\n",
        "        self.up8 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
        "        self.conv8 = DoubleConv(256, 128)\n",
        "        self.up9 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
        "        self.conv9 = DoubleConv(128, 64)\n",
        "        self.conv10 = nn.Conv2d(64, 34, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        c1=self.conv1(x)\n",
        "        p1=self.pool1(c1)\n",
        "        c2=self.conv2(p1)\n",
        "        p2=self.pool2(c2)\n",
        "        c3=self.conv3(p2)\n",
        "        p3=self.pool3(c3)\n",
        "        c4=self.conv4(p3)\n",
        "        p4=self.pool4(c4)\n",
        "        c5=self.conv5(p4)\n",
        "        up_6= self.up6(c5)\n",
        "        merge6 = torch.cat([up_6, c4], dim=1)\n",
        "        c6=self.conv6(merge6)\n",
        "        up_7=self.up7(c6)\n",
        "        merge7 = torch.cat([up_7, c3], dim=1)\n",
        "        c7=self.conv7(merge7)\n",
        "        up_8=self.up8(c7)\n",
        "        merge8 = torch.cat([up_8, c2], dim=1)\n",
        "        c8=self.conv8(merge8)\n",
        "        up_9=self.up9(c8)\n",
        "        merge9=torch.cat([up_9, c1], dim=1)\n",
        "        c9=self.conv9(merge9)\n",
        "        c10=self.conv10(c9)\n",
        "        out = c10  #nn.Sigmoid()(c10)\n",
        "        return out"
      ],
      "metadata": {
        "id": "hl6dWrLGLR8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "_Cr__dB5N2ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# transform testing datasets' H reduced img(0-420) to (512, 1024)\n",
        "def crop_tensor_test(x, y):\n",
        "    cropped_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((512, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    x_temp, y_temp = x[:, :, :420, :], y[:, :, :420, :]\n",
        "    x0, y0 = cropped_transform(x_temp[0]), cropped_transform(y_temp[0])\n",
        "    x1, y1 = cropped_transform(x_temp[1]), cropped_transform(y_temp[1])\n",
        "    x2, y2 = cropped_transform(x_temp[2]), cropped_transform(y_temp[2])\n",
        "    x3, y3 = cropped_transform(x_temp[3]), cropped_transform(y_temp[3])\n",
        "    x4, y4 = cropped_transform(x_temp[4]), cropped_transform(y_temp[4])\n",
        "    x_ = torch.cat((x0.unsqueeze(0), x1.unsqueeze(0), x2.unsqueeze(0), x3.unsqueeze(0), x4.unsqueeze(0)), 0)\n",
        "    y_ = torch.cat((y0.unsqueeze(0), y1.unsqueeze(0), y2.unsqueeze(0), y3.unsqueeze(0), y4.unsqueeze(0)), 0)  #(5, 3, 512, 1024)\n",
        "\n",
        "    x_, y_ = x_.cuda(), y_.cuda()\n",
        "\n",
        "\n",
        "    return (x_, y_)\n",
        "\n",
        "\n",
        "\n",
        "# show images\n",
        "def process_image(img):\n",
        "    img = img.cpu().data.numpy().transpose(1, 2, 0)\n",
        "    return img\n",
        "\n",
        "\n",
        "# save the training results\n",
        "def save_visualized_result(model, x, y, num_epoch):\n",
        "    (x_, y_) = crop_tensor_test(x, y)\n",
        "    predict_label = model(x_)\n",
        "    (temp_max, preds) = torch.max(predict_label.data, 1)  #preds->(5, 512, 1024)\n",
        "    #print(preds.shape)\n",
        "\n",
        "    fig, ax = plt.subplots(x_.size()[0], 3, figsize=(12, 10)) # figsize ratio (w,h) (18, 15)--->(5)\n",
        "\n",
        "    for i in range(x_.size()[0]):\n",
        "        ax[i, 0].get_xaxis().set_visible(False)\n",
        "        ax[i, 0].get_yaxis().set_visible(False)\n",
        "        ax[i, 1].get_xaxis().set_visible(False)\n",
        "        ax[i, 1].get_yaxis().set_visible(False)\n",
        "        ax[i, 2].get_xaxis().set_visible(False)\n",
        "        ax[i, 2].get_yaxis().set_visible(False)\n",
        "        ax[i, 0].cla()\n",
        "        ax[i, 0].imshow(process_image(x_[i]))\n",
        "        ax[i, 1].cla()\n",
        "        ax[i, 1].imshow(visualize_prediction(preds[i]))\n",
        "        ax[i, 2].cla()\n",
        "        ax[i, 2].imshow(process_image(y_[i]))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    label_epoch = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0, label_epoch, ha='center')\n",
        "    label_input = 'Input'\n",
        "    fig.text(0.18, 1, label_input, ha='center')\n",
        "    label_output = 'Output'\n",
        "    fig.text(0.5, 1, label_output, ha='center')\n",
        "    label_truth = 'Truth'\n",
        "    fig.text(0.81, 1, label_truth, ha='center')\n",
        "\n",
        "    plt.savefig(\n",
        "        'Training_results/Epoch %d.png' %\n",
        "        (num_epoch), bbox_inches='tight'\n",
        "    )#, bbox_inches='tight')\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "# visualize the results of categories  (512, 1024)->(512, 1024, 3)\n",
        "def visualize_prediction(output_model):\n",
        "    predict_img = output_model.cpu().data.numpy()\n",
        "    #print(predict_img)\n",
        "\n",
        "    color_array = np.array([\n",
        "        [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [111, 74, 0],\n",
        "        [81, 0, 81], [128, 64, 128], [244, 35, 232], [250, 170, 160], [230, 150, 140], [70, 70, 70],\n",
        "        [102, 102, 156], [190, 153, 153], [180, 165, 180], [150, 100, 100], [150, 120, 90], [153, 153, 153],\n",
        "        [153, 153, 153], [250, 170, 30], [220, 220, 0], [107, 142, 35], [152, 251, 152], [70, 130, 180],\n",
        "        [220, 20, 60], [255, 0, 0], [0, 0, 142], [0, 0, 70], [0, 60, 100], [0, 0, 90],\n",
        "        [0, 0, 110], [0, 80, 100], [0, 0, 230], [119, 11, 32]\n",
        "    ], dtype=np.uint8)\n",
        "\n",
        "    visualization = np.zeros((512, 1024, 3), dtype=np.uint8)\n",
        "    for h in range(predict_img.shape[0]):\n",
        "        for w in range(predict_img.shape[1]):\n",
        "            visualization[h][w] = color_array[(int)(predict_img[h][w])]\n",
        "            #visualization[h][w][1] = color_array[predict_img[h][w]][1]\n",
        "            #visualization[h][w][2] = color_array[predict_img[h][w]][2]\n",
        "\n",
        "    return visualization\n"
      ],
      "metadata": {
        "id": "zjX4gOVweUko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show the training result\n",
        "def show_result(model, x_, y_, num_epoch):\n",
        "    predict_images = model(x_)\n",
        "\n",
        "    fig, ax = plt.subplots(x_.size()[0], 3, figsize=(12, 10))\n",
        "\n",
        "    for i in range(x_.size()[0]):\n",
        "        ax[i, 0].get_xaxis().set_visible(False)\n",
        "        ax[i, 0].get_yaxis().set_visible(False)\n",
        "        ax[i, 1].get_xaxis().set_visible(False)\n",
        "        ax[i, 1].get_yaxis().set_visible(False)\n",
        "        ax[i, 2].get_xaxis().set_visible(False)\n",
        "        ax[i, 2].get_yaxis().set_visible(False)\n",
        "        ax[i, 0].cla()\n",
        "        ax[i, 0].imshow(process_image(x_[i]))\n",
        "        ax[i, 1].cla()\n",
        "        ax[i, 1].imshow(process_image(predict_images[i]))\n",
        "        ax[i, 2].cla()\n",
        "        ax[i, 2].imshow(process_image(y_[i]))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    label_epoch = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0, label_epoch, ha='center')\n",
        "    label_input = 'Input'\n",
        "    fig.text(0.18, 1, label_input, ha='center')\n",
        "    label_output = 'Output'\n",
        "    fig.text(0.5, 1, label_output, ha='center')\n",
        "    label_truth = 'Truth'\n",
        "    fig.text(0.81, 1, label_truth, ha='center')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# count parameters\n",
        "def count_params(model):\n",
        "    num_params = sum([item.numel() for item in model.parameters() if item.requires_grad])\n",
        "    return num_params\n",
        "\n",
        "# transform training datasets' H reduced img(0-420) to (512, 1024)\n",
        "def crop_tensor_train(x, y):\n",
        "    cropped_transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.Resize((512, 1024)),\n",
        "        transforms.ToTensor(),\n",
        "        # transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    batch_size = y.shape[0]\n",
        "    x_temp, y_temp = x[:, :, :420, :], y[:, :, :420, :]\n",
        "    x0, y0 = cropped_transform(x_temp[0]), cropped_transform(y_temp[0])\n",
        "    x0 = x0.unsqueeze(0)\n",
        "\n",
        "    # x_, y_ = x0.cuda(), y0.cuda()\n",
        "\n",
        "    x_, y_ = x0.to(device), y0.to(device)\n",
        "    return (x_, y_)\n",
        "\n",
        "# train process\n",
        "def train_model(model, criterion, optimizer, train_label_loader, train_raw_loader, test_raw, test_label, current_epoch=0, num_epochs=20):\n",
        "    start_time = time.time()\n",
        "    # loss history\n",
        "    hist_losses = []\n",
        "\n",
        "    for epoch in range(current_epoch, num_epochs):\n",
        "        print('Start training epoch %d' % (epoch + 1))\n",
        "        losses_list = []\n",
        "        epoch_start_time = time.time()\n",
        "        num_iter = 0\n",
        "        for (train_label, train_raw) in zip(train_label_loader, train_raw_loader):\n",
        "            y, temp1 = train_label  #torch.Size([1, 3, 512, 1024])\n",
        "            x, temp2 = train_raw  #torch.Size([1, 3, 512, 1024])\n",
        "            y = y.to(device)\n",
        "            temp1 = temp1.to(device)\n",
        "            x = x.to(device)\n",
        "            temp2 = temp2.to(device)\n",
        "\n",
        "            #print(y.shape)\n",
        "            #print(torch.sum(torch.sum(y, axis=2), axis=2))\n",
        "\n",
        "            x_, y_crop = crop_tensor_train(x, y)  #y_->(3, 512, 1024)\n",
        "            #print(y_.shape)\n",
        "            y_temp = y_crop[0].unsqueeze(0)\n",
        "            #print(y_temp.shape)\n",
        "            y_ = (y_temp * 255).long()\n",
        "            #print(x_.shape)\n",
        "            #print(y_.shape)\n",
        "            #print(torch.max(y_))\n",
        "\n",
        "            # Compute loss\n",
        "            # forward\n",
        "            outputs = model(x_)\n",
        "            #print(preds)\n",
        "            #print(preds.shape)\n",
        "            loss = criterion(outputs, y_)\n",
        "            # Bp and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            losses_list.append(loss)\n",
        "            hist_losses.append(loss)\n",
        "            num_iter += 1\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "\n",
        "        print('[%d/%d] - using time: %.2f' % ((epoch + 1), num_epochs, per_epoch_ptime))\n",
        "        print('Loss: %.3f' % (torch.mean(torch.FloatTensor(losses_list))))\n",
        "        \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            save_visualized_result(model, test_raw, test_label, (epoch + 1))\n",
        "\n",
        "        # save train result\n",
        "        torch.save(model.state_dict(), 'weights/weights_epoch%d.pth' % (epoch+1))\n",
        "        print(\"Weights saved!\")\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_ptime = end_time - start_time\n",
        "\n",
        "\n",
        "    return (model, hist_losses)\n",
        "\n",
        "\n",
        "# train the network\n",
        "def train(model, train_label_loader, train_raw_loader, test_raw, test_label,current_epoch=0, num_epochs=20):\n",
        "    # define LOSS functions\n",
        "    criterion = nn.CrossEntropyLoss()  #nn.BCELoss().cuda()\n",
        "    # Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.5, 0.999))  #lr = 1e-5  fair:0.0002 & (0.5, 0.999)\n",
        "    print('Training start!')\n",
        "    (model, hist_losses) = train_model(\n",
        "        model, criterion, optimizer,\n",
        "        train_label_loader, train_raw_loader, test_raw, test_label, current_epoch, num_epochs\n",
        "    )\n",
        "\n",
        "\n",
        "    return (model, hist_losses)\n"
      ],
      "metadata": {
        "id": "9HMa-izSVq2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(3, 34)\n",
        "print(device)\n",
        "model.load_state_dict(torch.load('./weights/weights_epoch1001.pth'))\n",
        "model = model.to(device)\n",
        "model.train()\n",
        "# report the architectures of Unet\n",
        "# print(model)\n",
        "print('Number of trainable parameters {}'.format(count_params(model)))\n",
        "# model.load_state_dict(torch.load('weights/weights_epoch10.pth'))\n",
        "\"\"\"\n",
        "wandb.init(\n",
        "      # Set the project where this run will be logged\n",
        "      project=\"U-Net\", \n",
        "      # Track hyperparameters and run metadata\n",
        "      config={\n",
        "      \"architecture\": \"UNet\",\n",
        "      \"dataset\": \"Cityscapes\",\n",
        "      \"epochs\": 105,\n",
        "})\n",
        "\"\"\"\n",
        "# train\n",
        "(model, hist_losses) = train(\n",
        "    model, train_label_loader, train_raw_loader, test_raw, test_label,current_epoch = 1000, num_epochs=1005\n",
        ")\n"
      ],
      "metadata": {
        "id": "kcBRxJrpN42f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(3, 34)\n",
        "print(device)\n",
        "model = model.to(device)\n",
        "model.load_state_dict(torch.load('./weights/weights_epoch1001.pth'))\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  save_visualized_result(model, test_raw, test_label, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot0Ft4kry6Ct",
        "outputId": "8ce7d897-3ded-42d1-9077-1490fc5e9145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "pSa6p5oHN5lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mIOU(target, prediction, num_class):\n",
        "    #Input: target & prediction: numpy H*\n",
        "    #Output: iou_scores: each classes' iou,ave: mIoU\n",
        "    iou_score = []\n",
        "    iou_scores = np.zeros(num_class,)\n",
        "    for i in range(num_class):\n",
        "        t = (target==i)\n",
        "        p = (prediction==i)\n",
        "        intersection = np.logical_and(t, p)\n",
        "        union = np.logical_or(t, p)\n",
        "        if np.sum(union) != 0:\n",
        "            iou_score.append(np.sum(intersection) / np.sum(union))\n",
        "            #iou_scores[i] = np.sum(intersection) / np.sum(union)\n",
        "        iou_scores[i] = np.sum(intersection) / np.sum(union)\n",
        "    ave = np.mean(iou_score)\n",
        "\n",
        "    return (iou_scores, ave)\n",
        "\n",
        "# resize image for compute accuracy (512, 1024, 3)->(1024, 2048, 3)\n",
        "def pyr_up_IOU(p):\n",
        "    p_ = p.copy()\n",
        "    return cv2.resize(p_, (2048, 1024), interpolation=cv2.INTER_NEAREST)"
      ],
      "metadata": {
        "id": "J5V6R4SMN6sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Unet(3, 34)\n",
        "model.load_state_dict(torch.load('./weights/weights_epoch1004.pth'))\n",
        "model = model.cuda()\n",
        "\n",
        "for i in range(0, 100):\n",
        "    n = str(i)\n",
        "    s = n.zfill(3)\n",
        "    print(s)\n",
        "    img_raw_path = './image_2/000'+s+'.png'\n",
        "\n",
        "    img_raw = Image.open(img_raw_path)\n",
        "    img_input = transform(img_raw).unsqueeze(0).to(device)\n",
        "    predict_label = model(img_input)\n",
        "\n",
        "    (temp_max, preds) = torch.max(predict_label.data, 1)  \n",
        "    preds_original = preds[0]\n",
        "    preds_original[preds_original <= 6] = 0\n",
        "    preds_original[(preds_original >= 7) & (preds_original <= 10)] = 7\n",
        "    preds_original[(preds_original >= 11) & (preds_original <= 16)] = 11\n",
        "    preds_original[(preds_original >= 17) & (preds_original <= 20)] = 17\n",
        "    preds_original[(preds_original >= 21) & (preds_original <= 22)] = 21\n",
        "    preds_original[(preds_original >= 24) & (preds_original <= 25)] = 24\n",
        "    preds_original[preds_original > 26] = 26\n",
        "\n",
        "    temp = Image.fromarray(visualize_prediction(preds_original)).resize((1242,375))\n",
        "\n",
        "    temp.save('./merge_kitti_rgb/000'+s+'.png')"
      ],
      "metadata": {
        "id": "kJ54qJ6Od2Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(len_test_label, test_label_loader) = load_data_labelID_test(\n",
        "    'data/gtFine',\n",
        "    'val', None, 1, shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "(len_test_raw, test_raw_loader) = load_data_raw(\n",
        "    'data/leftImg8bit',\n",
        "    'val', transform, 1, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "TFW-EQUbNK_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgPredicts = []\n",
        "imgLabels = []\n",
        "imgIoU = []\n",
        "model = Unet(3, 34)\n",
        "iou_scores_all = np.zeros(34,)\n",
        "\n",
        "model.load_state_dict(torch.load('./weights/weights_epoch1001.pth'))\n",
        "model = model.cuda()\n",
        "n = 0\n",
        "with torch.no_grad():\n",
        "  for (test_label, test_raw) in zip(test_label_loader, test_raw_loader):\n",
        "    \n",
        "    # print(test_label[0].shape) #torch.Size([5, 3, 512, 1024])\n",
        "    y, _ = test_label  \n",
        "    x, _ = test_raw  \n",
        "    # y = y.to(device)\n",
        "    x = x.to(device)\n",
        "\n",
        "    # print(x.shape)\n",
        "    # print(x)\n",
        "    predict_label = model(x)\n",
        "    (temp_max, preds) = torch.max(predict_label.data, 1)\n",
        "    imgPredict = pyr_up_IOU(preds[0].cpu().data.numpy())\n",
        "    imgLabel = np.array(y)\n",
        "    (iou_scores, ave) = mIOU(imgLabel, imgPredict, 34)\n",
        "    iou_scores[np.isnan(iou_scores)] = 0\n",
        "    print(iou_scores)\n",
        "    print(ave)\n",
        "    iou_scores_all += iou_scores\n",
        "    n += 1\n",
        "    # break\n",
        "\n",
        "  # (iou_scores, ave) = mIOU(imgLabel, imgPredict, 34)\n",
        "  # imgIoU.append(iou_scores)\n",
        "  \n",
        "  \n",
        "  # imgLabels.append(imgLabel)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DSxziZ11c1KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Label = namedtuple( 'Label' , [\n",
        "\n",
        "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
        "                    # We use them to uniquely name a class\n",
        "\n",
        "    'id'          , # An integer ID that is associated with this label.\n",
        "                    # The IDs are used to represent the label in ground truth images\n",
        "                    # An ID of -1 means that this label does not have an ID and thus\n",
        "                    # is ignored when creating ground truth images (e.g. license plate).\n",
        "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
        "                    # evaluation server.\n",
        "\n",
        "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
        "                    # ground truth images with train IDs, using the tools provided in the\n",
        "                    # 'preparation' folder. However, make sure to validate or submit results\n",
        "                    # to our evaluation server using the regular IDs above!\n",
        "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
        "                    # are mapped to the same class in the ground truth images. For the inverse\n",
        "                    # mapping, we use the label that is defined first in the list below.\n",
        "                    # For example, mapping all void-type classes to the same ID in training,\n",
        "                    # might make sense for some approaches.\n",
        "                    # Max value is 255!\n",
        "\n",
        "    'category'    , # The name of the category that this label belongs to\n",
        "\n",
        "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
        "                    # on category level.\n",
        "\n",
        "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
        "\n",
        "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
        "                    # during evaluations or not\n",
        "\n",
        "    'color'       , # The color of this label\n",
        "    ] )\n",
        "\n",
        "labels = [\n",
        "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
        "    Label(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
        "    Label(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
        "    Label(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
        "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
        "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
        "    Label(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
        "    Label(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
        "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
        "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
        "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
        "    Label(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
        "    Label(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
        "    Label(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
        "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
        "    Label(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
        "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
        "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
        "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
        "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
        "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
        "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
        "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
        "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
        "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
        "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
        "    Label(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
        "    Label(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
        "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
        "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
        "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
        "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
        "]"
      ],
      "metadata": {
        "id": "WbuHv_5DvyLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_label_map = {}\n",
        "iou_scores_all = list(iou_scores_all/n)\n",
        "for lable in labels:\n",
        "  class_label_map[lable.name] = iou_scores_all[lable.id]"
      ],
      "metadata": {
        "id": "20HtsjmZv1dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(class_label_map.items(), key=lambda item: item[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gqII2cv6csg",
        "outputId": "d7f27210-ef20-487c-9fa5-fc306b9b7b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('unlabeled', 0.0),\n",
              " ('tunnel', 0.0),\n",
              " ('polegroup', 0.00018896655891022308),\n",
              " ('rail track', 0.00029594764278822143),\n",
              " ('caravan', 0.0005389636375379433),\n",
              " ('trailer', 0.0006558250949273784),\n",
              " ('guard rail', 0.0007204225352112676),\n",
              " ('bridge', 0.002286358954961396),\n",
              " ('ego vehicle', 0.003591172527261949),\n",
              " ('train', 0.009844163072259915),\n",
              " ('truck', 0.02032375993474554),\n",
              " ('ground', 0.024548623142727353),\n",
              " ('motorcycle', 0.03189101395059155),\n",
              " ('dynamic', 0.03388884785619496),\n",
              " ('parking', 0.04319210060649177),\n",
              " ('bus', 0.0581834525965247),\n",
              " ('wall', 0.06947129359680092),\n",
              " ('fence', 0.07859418618848754),\n",
              " ('rectification border', 0.08779596365227266),\n",
              " ('rider', 0.10898917694043372),\n",
              " ('terrain', 0.13617073331951968),\n",
              " ('static', 0.13670155540171416),\n",
              " ('traffic light', 0.14964619726474673),\n",
              " ('bicycle', 0.24959546028473564),\n",
              " ('license plate', 0.24959546028473564),\n",
              " ('person', 0.3373705595352572),\n",
              " ('pole', 0.37438479739827385),\n",
              " ('traffic sign', 0.4217815570912375),\n",
              " ('sidewalk', 0.48234706658215676),\n",
              " ('out of roi', 0.5134921259087231),\n",
              " ('car', 0.6390125457909491),\n",
              " ('sky', 0.7048101069434228),\n",
              " ('building', 0.7468410800298971),\n",
              " ('road', 0.7911114775900767),\n",
              " ('vegetation', 0.8022626090610581)]"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iOhxXy8WjC4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}